2.1. BlazePalm Detector 
 To detect initial hand locations, we employ a single
 shot detector model optimized for mobile real-time appli
 cation similar to BlazeFace[1], which is also available in 
 MediaPipe[12]. Detecting hands is a decidedly complex 
 task: our model has to work across a variety of hand sizes 
 with a large scale span (∼20x) and be able to detect oc
 cluded and self-occluded hands. Whereas faces have high 
 contrast patterns, e.g., around the eye and mouth region, the 
 lack of such features in hands makes it comparatively diffifi- 
 cult to detect them reliably from their visual features alone. 
 Our solution addresses the above challenges using dif
 ferent strategies. 
 First, we train a palm detector instead of a hand detector, 
 since estimating bounding boxes of rigid objects like palms 
 and fifists is signifificantly simpler than detecting hands with 
 articulated fifingers. In addition, as palms are smaller ob
 jects, the non-maximum suppression algorithm works well 
 even for the two-hand self-occlusion cases, like handshakes. 
 Moreover, palms can be modelled using only square bound
 ing boxes [11], ignoring other aspect ratios, and therefore 
 reducing the number of anchors by a factor of 3∼5. 
 Second, we use an encoder-decoder feature extractor 
 similar to FPN[9] for a larger scene-context awareness even 
 for small objects. 
 Lastly, we minimize the focal loss[10] during training 
 to support a large amount of anchors resulting from the 
 high scale variance. High-level palm detector architecture 
 is shown in Figure 2. We present an ablation study of our 
 design elements in Table 1. 
  
 2.2. Hand Landmark Model
 After running palm detection over the whole image, our
 subsequent hand landmark model performs precise landmark localization of 21 2.5D coordinates inside the detected
 hand regions via regression. The model learns a consistent internal hand pose representation and is robust even to
 partially visible hands and self-occlusions. The model has
 three outputs (see Figure 3):
 1. 21 hand landmarks consisting of x, y, and relative
 depth.
 2. A hand flag indicating the probability of hand presence
 in the input image.
 3. A binary classification of handedness, e.g. left or right
 hand.
 We use the same topology as [14] for the 21 landmarks.
 The 2D coordinates are learned from both real-world images as well as synthetic datasets as discussed below, with
 the relative depth w.r.t. the wrist point being learned only
 from synthetic images. To recover from tracking failure, we
 developed another output of the model similar to [8] for producing the probability of the event that a reasonably aligned
 hand is indeed present in the provided crop. If the score is
 lower than a threshold then the detector is triggered to reset
 tracking. Handedness is another important attribute for effective interaction using hands in AR/VR. This is especially
 useful for some applications where each hand is associated
 with a unique functionality. Thus we developed a binary
 classification head to predict whether the input hand is the
 left or right hand. Our setup targets real-time mobile GPU
 inference, but we have also designed lighter and heavier versions of the model to address CPU inference on the mobile
 devices lacking proper GPU support and higher accuracy
 requirements of accuracy to run on desktop, respectively.
  
 3. Dataset and Annotation
 To obtain ground truth data, we created the following
 datasets addressing different aspects of the problem:
 • In-the-wild dataset: This dataset contains 6K images
 of large variety, e.g. geographical diversity, various
 lighting conditions and hand appearance. The limitation of this dataset is that it doesn’t contain complex
 articulation of hands.
 • In-house collected gesture dataset: This dataset contains 10K images that cover various angles of all physically possible hand gestures. The limitation of this
 dataset is that it’s collected from only 30 people with
 limited variation in background. The in-the-wild and
 in-house dataset are great complements to each other
 to improve robustness.
 • Synthetic dataset: To even better cover the possible hand poses and provide additional supervision for
 depth, we render a high-quality synthetic hand model
 over various backgrounds and map it to the corresponding 3D coordinates. We use a commercial 3D
 hand model that is rigged with 24 bones and includes
 36 blendshapes, which control fingers and palm thickness. The model also provides 5 textures with different skin tones. We created video sequences of transformation between hand poses and sampled 100K images from the videos. We rendered each pose with a
 random high-dynamic-range lighting environment and
 three different cameras. See Figure 4 for examples.
 For the palm detector, we only use in-the-wild dataset,
 which is sufficient for localizing hands and offers the highest variety in appearance. However, all datasets are used for
 training the hand landmark model. We annotate the realworld images with 21 landmarks and use projected groundtruth 3D joints for synthetic images. For hand presence, we
 select a subset of real-world images as positive examples
 and sample on the region excluding annotated hand regions
 as negative examples. For handedness, we annotate a subset
 of real-world images with handedness to provide such data.
  
 4. Results
 For the hand landmark model, our experiments show that
 the combination of real-world and synthetic datasets provides the best results. See Table 2 for details. We evaluate
 only on real-world images. Beyond the quality improvement, training with a large synthetic dataset leads to less
 jitter visually across frames. This observation leads us to
 believe that our real-world dataset can be enlarged for better generalization.
 Our target is to achieve real-time performance on mobile
 devices. We experimented with different model sizes and
 found that the “Full” model (see Table 3) provides a good
 trade-off between quality and speed. Increasing model capacity further introduces only minor improvements in quality but decreases significantly in speed (see Table 3 for details). We use the TensorFlow Lite GPU backend for ondevice inference[6].
  
 5. Implementation in MediaPipe
 With MediaPipe[12], our hand tracking pipeline can be
 built as a directed graph of modular components, called Calculators. Mediapipe comes with an extensible set of Calculators to solve tasks like model inference, media processing, and data transformations across a wide variety of devices and platforms. Individual Calculators like cropping,
 rendering and neural network computations are further optimized to utilize GPU acceleration. For example, we employ
 TFLite GPU inference on most modern phones.
 Our MediaPipe graph for hand tracking is shown in Figure 5. The graph consists of two subgraphs one for hand
 detection and another for landmarks computation. One key
 optimization MediaPipe provides is that the palm detector
 only runs as needed (fairly infrequently), saving significant
 computation. We achieve this by deriving the hand location
 in the current video frames from the computed hand landmarks in the previous frame, eliminating the need to apply
 the palm detector on every frame. For robustness, the hand
 tracker model also outputs an additional scalar capturing the
 confidence that a hand is present and reasonably aligned in
 the input crop. Only when the confidence falls below a certain threshold is the hand detection model reapplied to the
 next frame.
  
 6. Application examples
 Our hand tracking solution can readily be used in many
 applications such as gesture recognition and AR effects. On
 top of the predicted hand skeleton, we employ a simple algorithm to compute gestures, see Figure 6. First, the state
 of each finger, e.g. bent or straight, is determined via the accumulated angles of joints. Then, we map the set of finger
 states to a set of predefined gestures. This straightforward,
 yet effective technique allows us to estimate basic static gestures with reasonable quality. Beyond static gesture recognition, it is also possible to use a sequence of landmarks to
 predict dynamic gestures. Another application is to apply
 AR effects on top of the skeleton. Hand based AR effects
 currently enjoy high popularity. In Figure 7, we show an
 example AR rendering of the hand skeleton in neon light
 style.
  
 7. Conclusion
 In this paper, we proposed MediaPipe Hands, an end-toend hand tracking solution that achieves real-time performance on multiple platforms. Our pipeline predicts 2.5D
 landmarks without any specialized hardware and thus, can
 be easily deployed to commodity devices. We open sourced
 the pipeline to encourage researchers and engineers to build
 gesture control and creative AR/VR applications with our
 pipeline.  